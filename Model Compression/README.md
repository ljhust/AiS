| Title | Remark |
| :---- | :----: |
| [Awesome-CoreML-Models](https://github.com/likedan/Awesome-CoreML-Models)|
|[distiller](https://github.com/NervanaSystems/distiller)|Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.|
|[QNNPACK](https://github.com/pytorch/QNNPACK)|Quantized Neural Network PACKage - mobile-optimized implementation of quantized neural network operators|
|[PocketFlow](https://github.com/Tencent/PocketFlow)|An Automatic Model Compression (AutoMC) framework for developing smaller and faster AI applications. |
|[awesome-model-compression-and-acceleration](https://github.com/sun254/awesome-model-compression-and-acceleration)|
|[Awesome-Pruning](https://github.com/he-y/Awesome-Pruning)|A curated list of neural network pruning resources.|
|[gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)|Make huge neural nets fit in memory|
|[ELL](https://github.com/Microsoft/ELL)|The Embedded Learning Library (ELL) allows you to design and deploy intelligent machine-learned models onto resource constrained platforms and small single-board computers, like Raspberry Pi, Arduino, and micro:bit. |
|[awesome-emdl](https://github.com/EMDL/awesome-emdl)|Embedded and mobile deep learning research resources|
|[apex](https://github.com/nvidia/apex)|A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch|
|[FeatherCNN](https://github.com/Tencent/FeatherCNN)|FeatherCNN is a high performance inference engine for convolutional neural networks.|
|[sru](https://github.com/asappresearch/sru)|Training RNNs as Fast as CNNs|
|[awesome-quantum-machine-learning](https://github.com/krishnakumarsekar/awesome-quantum-machine-learning)|
|[hyperlearn](https://github.com/danielhanchen/hyperlearn)|50% faster, 50% less RAM Machine Learning. Numba rewritten Sklearn. SVD, NNMF, PCA, LinearReg, RidgeReg, Randomized, Truncated SVD/PCA, CSR Matrices all 50+% faster |
|[webdnn](https://github.com/mil-tokyo/webdnn)|The Fastest DNN Running Framework on Web Browser|
|[glow](https://github.com/pytorch/glow/)|Compiler for Neural Network hardware accelerators|
|[TurboTransformers](https://github.com/Tencent/TurboTransformers)|a fast and user-friendly tool for transformer inference on CPU and GPU|
|[POT](https://github.com/PythonOT/POT)|Python Optimal Transport library https://PythonOT.github.io/|
|[aimet](https://github.com/quic/aimet)|AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.|
|[open_lth](https://github.com/facebookresearch/open_lth)|A repository in preparation for open-sourcing lottery ticket hypothesis code.|
|[checkmate](https://github.com/parasj/checkmate?u=1661452664&m=4511458685814429&cu=1661452664)|Training neural networks in TensorFlow 2.0 with 5x less memory|
|[wezterm](https://github.com/wez/wezterm)|A GPU-accelerated cross-platform terminal emulator and multiplexer written by @wez and implemented in Rust https://wezfurlong.org/wezterm/|