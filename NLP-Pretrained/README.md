| Title | Viewd | Remark |
| :---- | :----: | :----: |
| [BERT-related-papers](https://github.com/tomohideshibata/BERT-related-papers) | *No* | BERT-related papers  |
| [bert-extractive-summarizer](https://github.com/dmmiller612/bert-extractive-summarizer) | *NO* |  NA  |
| [ALBERT](https://github.com/google-research/ALBERT) | *NO* | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations |
| [transformers](https://github.com/sannykim/transformers) | *NO* | A collection of resources to study Transformers in depth. |
| [All The Ways You Can Compress BERT](http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html) | *NO* | a review of BERT blog |
| [bert lang street](https://bertlang.unibocconi.it/) | *NO* |the bert model collections hub |
| [huggingface pretraintd NPL models](https://huggingface.co/models) | *NO* | all models and checkpoints |
|[transformers](https://github.com/huggingface/transformers)|ðŸ¤— Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. https://huggingface.co/transformers|
|[bertviz](https://github.com/jessevig/bertviz)|Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)|
|[gpt-2](https://github.com/openai/gpt-2)|Code for the paper "Language Models are Unsupervised Multitask Learners" |


