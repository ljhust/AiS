| Title | Viewd | Remark |
| :---- | :----: | :----: |
| [BERT-related-papers](https://github.com/tomohideshibata/BERT-related-papers) | *No* | BERT-related papers  |
| [bert-extractive-summarizer](https://github.com/dmmiller612/bert-extractive-summarizer) | *NO* |  NA  |
| [ALBERT](https://github.com/google-research/ALBERT) | *NO* | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations |
| [transformers](https://github.com/sannykim/transformers) | *NO* | A collection of resources to study Transformers in depth. |
| [All The Ways You Can Compress BERT](http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html) | *NO* | a review of BERT blog |
| [bert lang street](https://bertlang.unibocconi.it/) | *NO* |the bert model collections hub |
| [huggingface pretraintd NPL models](https://huggingface.co/models) | *NO* | all models and checkpoints |
|[transformers](https://github.com/huggingface/transformers)|ğŸ¤— Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. https://huggingface.co/transformers|
|[bertviz](https://github.com/jessevig/bertviz)|Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)|
|[gpt-2](https://github.com/openai/gpt-2)|Code for the paper "Language Models are Unsupervised Multitask Learners" |
|[FinBERT](https://github.com/valuesimplex/FinBERT)|é¦–ä¸ªåœ¨é‡‘èé¢†åŸŸå¤§è§„æ¨¡è¯­æ–™ä¸Šè®­ç»ƒçš„å¼€æºä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹|
|[sentence-transformers](https://github.com/UKPLab/sentence-transformers)|Multilingual Sentence & Image Embeddings with BERT|
|[awesome-gpt3](https://github.com/elyase/awesome-gpt3)||
|[Awesome-CLIP](https://github.com/yzhuoning/Awesome-CLIP)|Awesome list for research on CLIP (Contrastive Language-Image Pre-Training).|
